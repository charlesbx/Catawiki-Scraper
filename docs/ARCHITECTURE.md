# ğŸ—ï¸ Architecture Documentation

## Overview

The Catawiki Scraper follows a **modular, layered architecture** designed for maintainability, testability, and scalability. The system is organized into distinct modules with clear responsibilities and minimal coupling.

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Entry Points (scripts/)                  â”‚
â”‚  scrape_listings.py | monitor_deals.py | check_items.py    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Application Layer                          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Scraper    â”‚  â”‚   Analyzer   â”‚  â”‚ Notification â”‚    â”‚
â”‚  â”‚   Module     â”‚â”€â”€â”‚   Module     â”‚â”€â”€â”‚   Module     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Storage & Data Models                     â”‚   â”‚
â”‚  â”‚        (JSON Store, WatchItem, DealAlert)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚        Utilities (Logger, Time Utils, Config)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  External Services                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Catawiki    â”‚  â”‚   Telegram   â”‚  â”‚  File System â”‚    â”‚
â”‚  â”‚  Website     â”‚  â”‚     Bot      â”‚  â”‚  (JSON/Logs) â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Directory Structure

```
catawiki-scraper/
â”œâ”€â”€ src/                          # Source code modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper/                  # Web scraping logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ browser.py            # Browser management
â”‚   â”‚   â”œâ”€â”€ parser.py             # HTML parsing (TODO)
â”‚   â”‚   â””â”€â”€ watch_scraper.py      # Main scraping logic (TODO)
â”‚   â”œâ”€â”€ analyzer/                 # Deal analysis
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ filters.py            # Deal filtering & scoring
â”‚   â”œâ”€â”€ notifications/            # Alert system
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ telegram.py           # Telegram client
â”‚   â”œâ”€â”€ storage/                  # Data persistence
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ json_store.py         # JSON file operations
â”‚   â”‚   â””â”€â”€ models.py             # Data models (WatchItem, etc.)
â”‚   â”œâ”€â”€ config/                   # Configuration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py           # Environment-based config
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py             # Logging setup
â”‚       â””â”€â”€ time_utils.py         # Time parsing functions
â”œâ”€â”€ scripts/                      # Entry point scripts
â”‚   â”œâ”€â”€ scrape_listings.py        # Initial scraping
â”‚   â”œâ”€â”€ monitor_deals.py          # Continuous monitoring
â”‚   â””â”€â”€ check_items.py            # Item verification
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ test_analyzer.py
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # This file
â”‚   â””â”€â”€ API.md                    # API documentation
â”œâ”€â”€ config.py                     # Legacy config (deprecated)
â”œâ”€â”€ main.py                       # Legacy main (deprecated)
â”œâ”€â”€ .env.example                  # Environment template
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # Project overview
```

## Module Responsibilities

### 1. Scraper Module (`src/scraper/`)

**Purpose:** Handle all web scraping operations.

**Components:**
- `browser.py`: Manages Chrome/Chromium browser instances
  - Configuration (headless, user agent, etc.)
  - Driver lifecycle management
  - Context manager support
  
- `parser.py` (TODO): HTML parsing and data extraction
  - BeautifulSoup integration
  - CSS selector definitions
  - Data normalization

- `watch_scraper.py` (TODO): Main scraping orchestration
  - Pagination handling
  - Infinite scroll
  - Item extraction pipeline

**Key Design Decisions:**
- Separation of browser management from parsing logic
- Context manager pattern for resource cleanup
- Configurable browser options via environment

### 2. Analyzer Module (`src/analyzer/`)

**Purpose:** Analyze items to identify good deals.

**Components:**
- `filters.py`: Deal detection and scoring
  - `DealCriteria`: Configurable filtering thresholds
  - `DealAnalyzer`: Main analysis engine
  - Price ratio calculations
  - Time-based urgency detection
  - Deal quality scoring

**Key Features:**
- Configurable thresholds (price %, time remaining)
- Reserve price filtering
- Multiple filtering strategies
- Deal quality scoring (0.0 = best, 1.0 = worst)

### 3. Notifications Module (`src/notifications/`)

**Purpose:** Send alerts to users.

**Components:**
- `telegram.py`: Telegram Bot API integration
  - Async message sending
  - Multi-recipient support
  - Error handling per recipient

**Design Patterns:**
- Async/await for non-blocking I/O
- Graceful degradation (continues if one recipient fails)

### 4. Storage Module (`src/storage/`)

**Purpose:** Data persistence and models.

**Components:**
- `models.py`: Data classes
  - `WatchItem`: Represents auction listing
  - `DealAlert`: Notification representation
  - Helper methods for price parsing
  
- `json_store.py`: JSON file operations
  - CRUD operations
  - Automatic backups
  - URL-based lookups
  - Safe file operations

**Key Features:**
- Dataclass-based models (immutable, type-safe)
- Automatic backup before write
- URL-based indexing
- Type-safe conversions (to_dict, from_dict)

### 5. Config Module (`src/config/`)

**Purpose:** Centralized configuration management.

**Components:**
- `settings.py`: Environment-based config
  - Loads from `.env` file
  - Type conversions (str â†’ int, float, bool)
  - Validation of required values
  - Default values for optional settings

**Configuration Sources:**
1. `.env` file (primary)
2. Environment variables
3. Hardcoded defaults (fallback)

### 6. Utils Module (`src/utils/`)

**Purpose:** Shared utilities across modules.

**Components:**
- `logger.py`: Logging configuration
  - Console + file handlers
  - Configurable log levels
  - Structured formatting
  
- `time_utils.py`: Time parsing
  - French time format parsing (1j 5h 30m)
  - Remaining time calculations
  - Seconds â†” human-readable conversion

## Data Flow

### 1. Scraping Flow

```
User runs script
    â†“
BrowserManager creates driver
    â†“
Scraper navigates to Catawiki
    â†“
Parser extracts item data
    â†“
WatchItem objects created
    â†“
JSONStorage saves items
    â†“
Browser closed
```

### 2. Deal Detection Flow

```
Load items from JSONStorage
    â†“
For each item:
    â†“
    DealAnalyzer.is_good_deal()
        â”œâ”€ Check price vs estimate
        â”œâ”€ Check remaining time
        â””â”€ Check reserve status
    â†“
Filter good deals
    â†“
Sort by deal quality
    â†“
Return filtered list
```

### 3. Notification Flow

```
Good deals identified
    â†“
For each deal:
    â†“
    Format message
    â†“
    Create DealAlert
    â†“
    send_telegram_message()
        â”œâ”€ Send to recipient 1
        â”œâ”€ Send to recipient 2
        â””â”€ ...
    â†“
Log success/failure
```

## Design Patterns

### 1. **Context Manager Pattern**
Used in `BrowserManager` for automatic resource cleanup:
```python
with BrowserManager() as driver:
    # Use driver
    pass
# Automatically closed
```

### 2. **Dataclass Pattern**
Used in `models.py` for type-safe, immutable data:
```python
@dataclass
class WatchItem:
    title: str
    price: str
    # ...
```

### 3. **Strategy Pattern**
`DealCriteria` allows different filtering strategies:
```python
criteria = DealCriteria(
    price_threshold=0.80,  # 80% instead of 90%
    time_threshold=3600    # 1 hour instead of 30min
)
analyzer = DealAnalyzer(criteria)
```

### 4. **Repository Pattern**
`JSONStorage` abstracts data access:
```python
storage = JSONStorage("items.json")
items = storage.load()
storage.save(updated_items)
```

## Error Handling Strategy

### 1. **Graceful Degradation**
- Telegram send failures don't stop other recipients
- Parser errors skip item, continue with next

### 2. **Logging**
- All errors logged with context
- Different log levels (DEBUG, INFO, WARNING, ERROR)

### 3. **Validation**
- Config validation on startup
- Data validation in models
- Type hints for compile-time checks

## Testing Strategy

### 1. **Unit Tests**
- Individual functions (e.g., `get_total_seconds`)
- Pure logic (no external dependencies)

### 2. **Integration Tests**
- Module interactions (e.g., DealAnalyzer + JSONStorage)
- Mocked external services

### 3. **E2E Tests**
- Full scraping pipeline (with mocked website)
- Notification flow

## Performance Considerations

### 1. **Browser Optimization**
- Headless mode reduces overhead
- Image loading disabled
- Minimal wait times

### 2. **Data Efficiency**
- JSON for fast read/write
- In-memory filtering (no DB overhead)
- Backup only when data changes

### 3. **Network**
- Rate limiting (TODO)
- Retry logic with backoff (TODO)
- Connection pooling (TODO)

## Security

### 1. **Credentials**
- Never committed to Git
- Environment variables only
- `.env` in `.gitignore`

### 2. **Input Validation**
- URL validation
- Price parsing with error handling
- Type checking everywhere

### 3. **Dependencies**
- Pinned versions in `requirements.txt`
- Regular security updates

## Future Enhancements

### 1. **Database Backend**
- Replace JSON with SQLite/PostgreSQL
- Indexed queries for performance
- Historical data tracking

### 2. **API Layer**
- RESTful API for remote access
- Authentication & authorization
- Rate limiting

### 3. **Distributed Architecture**
- Separate scraper, analyzer, notifier
- Message queue (RabbitMQ/Redis)
- Horizontal scaling

### 4. **Monitoring**
- Health checks
- Metrics (Prometheus)
- Dashboards (Grafana)

## Backward Compatibility

Legacy files maintained temporarily for migration:
- `config.py` â†’ `src/config/settings.py`
- `utils.py` â†’ `src/utils/time_utils.py`
- `main.py` â†’ `scripts/scrape_listings.py`

**Deprecation Timeline:**
- **Phase 1** (Current): Both old and new coexist
- **Phase 2** (Next): Old files marked deprecated
- **Phase 3** (Future): Old files removed

---

**Document Version:** 2.0.0  
**Last Updated:** 2025-11-19  
**Maintainer:** Charles Baux
